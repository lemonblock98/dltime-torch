{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_process import *\n",
    "from data_split import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data...\n",
      "generating data finishing...\n"
     ]
    }
   ],
   "source": [
    "snr=0\n",
    "print(\"generating data...\")\n",
    "data_root = 'D:/研一/嗑盐/土壤扰动/dataset/'\n",
    "data_len = 64\n",
    "syf_train, syf_test, _, _ = generate_data(data_root + 'syf', by_txt=False, snr=snr, data_len=data_len)\n",
    "syf2_train, syf2_test, _, _ = generate_data(data_root + 'syf2', by_txt=False, snr=snr, data_len=data_len)\n",
    "\n",
    "yqcc_train, yqcc_test, _, _ = generate_data(data_root + 'yqcc2', by_txt=False, snr=snr, data_len=data_len)\n",
    "yqcc2_train, yqcc2_test, _, _ = generate_data(data_root + 'yqcc2_md', by_txt=False, snr=snr, data_len=data_len)\n",
    "\n",
    "zwy_train, zwy_test, _, _ = generate_data(data_root + 'zwy', by_txt=False, snr=snr, data_len=data_len)\n",
    "zwy2_train, zwy2_test, _, _ = generate_data(data_root + 'zwy_d1', by_txt=False, snr=snr, data_len=data_len)\n",
    "zwy3_train, zwy3_test, _, _ = generate_data(data_root + 'zwy_418', by_txt=False, snr=snr, data_len=data_len)\n",
    "zwy4_train, zwy4_test, _, _ = generate_data(data_root + 'zwy_423', by_txt=False, snr=snr, data_len=data_len)\n",
    "zwy5_train, zwy5_test, _, _ = generate_data(data_root + 'zwy_621', by_txt=False, snr=snr, data_len=data_len)\n",
    "\n",
    "j11_train, j11_test, _, _ = generate_data(data_root + 'j11', by_txt=False, snr=snr, data_len=data_len)\n",
    "j11_2_train, j11_2_test, _, _ = generate_data(data_root + 'j11_328', by_txt=False, snr=snr, data_len=data_len)\n",
    "j11_md_train, j11_md_test, _, _ = generate_data(data_root + 'j11_49', by_txt=False, snr=snr, data_len=data_len)\n",
    "j11_527_train, j11_527_test, _, _ = generate_data(data_root + 'j11_527', by_txt=False, snr=snr, data_len=data_len)\n",
    "j11_709_train, j11_709_test, _, _ = generate_data(data_root + 'j11_709', by_txt=False, snr=snr, data_len=data_len, resample=False)\n",
    "j11_717_train, j11_717_test, _, _ = generate_data(data_root + 'j11_717', by_txt=False, snr=snr, data_len=data_len, resample=False)\n",
    "\n",
    "zyq_train, zyq_test, _, _ = generate_data(data_root + 'zyq', by_txt=False, snr=snr, data_len=data_len)\n",
    "zyq2_train, zyq2_test, _, _ = generate_data(data_root + 'zyq_d1', by_txt=False, snr=snr, data_len=data_len)\n",
    "\n",
    "j7lqc_train, j7lqc_test, _, _ = generate_data(data_root + 'j7lqc', by_txt=False, snr=snr, data_len=data_len)\n",
    "sky_train, sky_test, _, _ = generate_data(data_root + 'sky', by_txt=False, snr=snr, data_len=data_len)\n",
    "sky2_train, sky2_test, _, _ = generate_data(data_root + 'sky_531', by_txt=False, snr=snr, data_len=data_len, resample=False)\n",
    "sky3_train, sky3_test, _, _ = generate_data(data_root + 'sky_617', by_txt=False, snr=snr, data_len=data_len, resample=False)\n",
    "print(\"generating data finishing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./pickle_data'):\n",
    "    os.makedirs('./pickle_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(data, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_pkl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './pickle_data'\n",
    "save_pkl(sky2_train, SAVE_PATH + '/sky2_train.pkl')\n",
    "save_pkl(sky2_test, SAVE_PATH + '/sky2_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky2_train_A = [item for item in sky2_train if 'A' in item['file_name']]\n",
    "sky2_train_B = [item for item in sky2_train if 'B' in item['file_name']]\n",
    "\n",
    "sky2_test_A = [item for item in sky2_test if 'A' in item['file_name']]\n",
    "sky2_test_B = [item for item in sky2_test if 'B' in item['file_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './pickle_data'\n",
    "save_pkl(sky2_train_A, SAVE_PATH + '/sky2_train_A.pkl')\n",
    "save_pkl(sky2_train_B, SAVE_PATH + '/sky2_train_B.pkl')\n",
    "save_pkl(sky2_test_A, SAVE_PATH + '/sky2_test_A.pkl')\n",
    "save_pkl(sky2_test_B, SAVE_PATH + '/sky2_test_B.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './pickle_data'\n",
    "\n",
    "# syf\n",
    "save_pkl(syf_train, SAVE_PATH + f'/syf_train_{data_len}.pkl')\n",
    "save_pkl(syf_test, SAVE_PATH + f'/syf_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(syf2_train, SAVE_PATH + f'/syf2_train_{data_len}.pkl')\n",
    "save_pkl(syf2_test, SAVE_PATH + f'/syf2_test_{data_len}.pkl')\n",
    "\n",
    "# yqcc\n",
    "save_pkl(yqcc_train, SAVE_PATH + f'/yqcc_train_{data_len}.pkl')\n",
    "save_pkl(yqcc_test, SAVE_PATH + f'/yqcc_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(yqcc2_train, SAVE_PATH + f'/yqcc2_train_{data_len}.pkl')\n",
    "save_pkl(yqcc2_test, SAVE_PATH + f'/yqcc2_test_{data_len}.pkl')\n",
    "\n",
    "# zwy\n",
    "save_pkl(zwy_train, SAVE_PATH + f'/zwy_train_{data_len}.pkl')\n",
    "save_pkl(zwy_test, SAVE_PATH + f'/zwy_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(zwy2_train, SAVE_PATH + f'/zwy2_train_{data_len}.pkl')\n",
    "save_pkl(zwy2_test, SAVE_PATH + f'/zwy2_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(zwy3_train, SAVE_PATH + f'/zwy3_train_{data_len}.pkl')\n",
    "save_pkl(zwy3_test, SAVE_PATH + f'/zwy3_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(zwy4_train, SAVE_PATH + f'/zwy4_train_{data_len}.pkl')\n",
    "save_pkl(zwy4_test, SAVE_PATH + f'/zwy4_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(zwy5_train, SAVE_PATH + f'/zwy5_train_{data_len}.pkl')\n",
    "save_pkl(zwy5_test, SAVE_PATH + f'/zwy5_test_{data_len}.pkl')\n",
    "\n",
    "# j11\n",
    "save_pkl(j11_train, SAVE_PATH + f'/j11_train_{data_len}.pkl')\n",
    "save_pkl(j11_test, SAVE_PATH + f'/j11_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(j11_2_train, SAVE_PATH + f'/j11_2_train_{data_len}.pkl')\n",
    "save_pkl(j11_2_test, SAVE_PATH + f'/j11_2_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(j11_md_train, SAVE_PATH + f'/j11_md_train_{data_len}.pkl')\n",
    "save_pkl(j11_md_test, SAVE_PATH + f'/j11_md_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(j11_527_train, SAVE_PATH + f'/j11_527_train_{data_len}.pkl')\n",
    "save_pkl(j11_527_test, SAVE_PATH + f'/j11_527_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(j11_709_train, SAVE_PATH + f'/j11_709_train_{data_len}.pkl')\n",
    "save_pkl(j11_709_test, SAVE_PATH + f'/j11_709_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(j11_717_train, SAVE_PATH + f'/j11_717_train_{data_len}.pkl')\n",
    "save_pkl(j11_717_test, SAVE_PATH + f'/j11_717_test_{data_len}.pkl')\n",
    "\n",
    "# zyq\n",
    "save_pkl(zyq_train, SAVE_PATH + f'/zyq_train_{data_len}.pkl')\n",
    "save_pkl(zyq_test, SAVE_PATH + f'/zyq_test_{data_len}.pkl')\n",
    "\n",
    "save_pkl(zyq2_train, SAVE_PATH + f'/zyq2_train_{data_len}.pkl')\n",
    "save_pkl(zyq2_test, SAVE_PATH + f'/zyq2_test_{data_len}.pkl')\n",
    "\n",
    "# sky\n",
    "save_pkl(sky_train, SAVE_PATH + f'/sky_train_{data_len}.pkl')\n",
    "save_pkl(sky_test, SAVE_PATH + f'/sky_test_{data_len}.pkl')\n",
    "save_pkl(sky2_train, SAVE_PATH + f'/sky2_train_{data_len}.pkl')\n",
    "save_pkl(sky2_test, SAVE_PATH + f'/sky2_test_{data_len}.pkl')\n",
    "save_pkl(sky3_train, SAVE_PATH + f'/sky3_train_{data_len}.pkl')\n",
    "save_pkl(sky3_test, SAVE_PATH + f'/sky3_test_{data_len}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_pkl(SAVE_PATH + '/sky3_train_64.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_train = ['zwy', 'zwy2', 'zwy3', 'zwy4', 'zwy5']\n",
    "train_data = []\n",
    "test_data = []\n",
    "for data_name in data_for_train:\n",
    "    train_data.extend(load_pkl(f'./pickle_data/{data_name}_train.pkl'))\n",
    "    test_data.extend(load_pkl(f'./pickle_data/{data_name}_test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (4671, 64, 5) (4671,)\n"
     ]
    }
   ],
   "source": [
    "# data process\n",
    "from sklearn.utils import shuffle\n",
    "data_for_train = ['zwy', 'zwy2', 'zwy3', 'zwy4']\n",
    "train_data = []\n",
    "test_data = []\n",
    "for data_name in data_for_train:\n",
    "    train_data.extend(load_pkl(f'./pickle_data/{data_name}_train_{data_len}.pkl'))\n",
    "    test_data.extend(load_pkl(f'./pickle_data/{data_name}_test_{data_len}.pkl'))\n",
    "\n",
    "train_data = shuffle(train_data)\n",
    "train_x, train_label = handle_dataset_3dims(train_data, mode=\"all\")\n",
    "test_x, test_label = handle_dataset_3dims(test_data, mode=\"all\")\n",
    "train_x = np.swapaxes(train_x, 2, 1)\n",
    "test_x = np.swapaxes(test_x, 2, 1)\n",
    "print('Train data size:', train_x.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 2, 2, 1, 2, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "data_for_train = ['zwy', 'zwy2', 'zwy3', 'zwy4', 'j11', 'j11_2', 'j11_md', 'j11_527', 'yqcc', 'yqcc2', 'sky', 'syf', 'syf2', 'zyq', 'zyq2']\n",
    "# data_for_train = ['zwy', 'zwy2', 'zwy3', 'zwy4']\n",
    "# data_for_train = ['sky']\n",
    "train_data = []\n",
    "test_data = []\n",
    "for data_name in data_for_train:\n",
    "    train_data.extend(load_pkl(f'./pickle_data/{data_name}_train_64.pkl'))\n",
    "    test_data.extend(load_pkl(f'./pickle_data/{data_name}_test_64.pkl'))\n",
    "\n",
    "# train_data = shuffle(train_data)\n",
    "train_x, train_label = handle_dataset_3dims(train_data, mode=\"norm\")\n",
    "test_x, test_label = handle_dataset_3dims(test_data, mode=\"norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8182093304578673\n",
      "3.167749956229134e-06\n",
      "0.7947659415040964\n",
      "2.1073424255447017e-07\n",
      "0.7506234158284075\n",
      "-1.4712622773203075\n",
      "0.37584662437438965\n",
      "-0.5033724308013915\n",
      "0.7856737673282623\n",
      "-0.45967864990234375\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_x[:, 0, :]))\n",
    "print(np.min(train_x[:, 0, :]))\n",
    "\n",
    "print(np.max(train_x[:, 1, :]))\n",
    "print(np.min(train_x[:, 1, :]))\n",
    "\n",
    "print(np.max(train_x[:, 2, :]))\n",
    "print(np.min(train_x[:, 2, :]))\n",
    "\n",
    "print(np.max(train_x[:, 3, :]))\n",
    "print(np.min(train_x[:, 3, :]))\n",
    "\n",
    "print(np.max(train_x[:, 4, :]))\n",
    "print(np.min(train_x[:, 4, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JudgeModule(x:Tensor):\n",
    "    # fc_cor=FC(x[0,:,0])\n",
    "    x=x.unsqueeze(3)\n",
    "    b, c, _ ,_= x.size()\n",
    "    x_fft,fft_init=transfft(x)\n",
    "    k1=torch.Tensor(np.arange(1,x_fft.shape[2]+1)).to(device).repeat(x_fft.shape[0],x_fft.shape[1],1)\n",
    "    fc=k1.unsqueeze(3)*x_fft\n",
    "    y_1=(torch.sum(fc,dim=2)/torch.sum(x_fft,dim=2)).view(b,c,1,1) #fc\n",
    "    y_11 = nn.AdaptiveAvgPool2d(1)(x_fft[:,:,:]).view(b,c,1,1)\n",
    "    y_22=nn.AdaptiveMaxPool2d(1)(abs(x_fft[:,:,:])).view(b,c,1,1)\n",
    "    y_2=y_22/y_11 #峰值因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T2FPlanner(each,path,model,logger):\n",
    "    min_skip=2 #最多跳过几个 3-1\n",
    "    fftout,fcout=model.judgefft()\n",
    "    num=0\n",
    "    layer_gain=[] #跳过\n",
    "    layer_capacity=[] #加深\n",
    "    resultdf2=pd.DataFrame()\n",
    "    for i in fftout:\n",
    "        y11=i\n",
    "        y11=torch.where(torch.isnan(y11), torch.full_like(y11, 0), y11)\n",
    "        k=torch.mean(y11,dim=1)\n",
    "        kk=torch.mean(k,dim=0)\n",
    "        kkk=torch.max(y11,dim=1)[0].data #最大值\n",
    "        kkkk=torch.max(kkk,dim=0)[0].data #最大值             \n",
    "        _k = torch.var(y11, dim=1,unbiased=False) \n",
    "        var_k=torch.mean(_k,dim=0)\n",
    "        resultdf2=resultdf2.append([[each,kk.squeeze().cpu().item(),var_k.squeeze().cpu().item()]],ignore_index=True)\n",
    "\n",
    "        if num==0: #self.out[0]第一层卷积后的结果\n",
    "            y22=torch.where(torch.isnan(fcout[0]), torch.full_like(fcout[0], 0), fcout[0])\n",
    "            fc_avg=torch.mean(y22,dim=1)\n",
    "            fc_avg=torch.mean(fc_avg,dim=0)\n",
    "            logger.info(\"CONV1_fc\"+str(fc_avg))\n",
    "            print(\"CONV1_fc\",fc_avg)\n",
    "        elif num==1: #第一个卷积后的增益\n",
    "            last_avg=kk.squeeze().cpu().item()\n",
    "            last_var=var_k.squeeze().cpu().item()\n",
    "            logger.info(str(num)+\":\"+str(last_var))\n",
    "        else:\n",
    "            gain=last_avg-kk.squeeze().cpu().item()\n",
    "            capacity=last_var -var_k.squeeze().cpu().item()\n",
    "            last_avg=kk.squeeze().cpu().item()\n",
    "            last_var=var_k.squeeze().cpu().item()\n",
    "            layer_gain.append(gain) #跳过\n",
    "            layer_capacity.append(capacity) #var差值\n",
    "            logger.info(str(num)+\":\"+str(last_var))\n",
    "            print(num,last_var)\n",
    "        num=num+1\n",
    "\n",
    "    print(resultdf2)\n",
    "    print(\"CONV1_fc\",fc_avg)\n",
    "    print(layer_gain,layer_capacity)\n",
    "    resultdf2=resultdf2.append([[fc_avg,layer_gain,layer_capacity]],ignore_index=True)\n",
    "    resultdf2.to_csv(path,index=False) #modellog_fz_l\n",
    "    layer_capacity_pre = [i for i in layer_capacity]\n",
    "    skip=[]\n",
    "    while True:\n",
    "        if len(layer_capacity)==0 or len(layer_capacity)==1:\n",
    "            return False,[],[],None\n",
    "\n",
    "        x=min(layer_capacity)\n",
    "        if x<0 and len(skip)<min_skip:\n",
    "            find=(layer_capacity.index(x))\n",
    "            skip.append(find+1) #第一个block不跳，capacity从第二个layer开始，序号为1，返回为第一层\n",
    "            layer_capacity[find]=0\n",
    "        else:\n",
    "            return False,skip,layer_capacity_pre,fc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2cc5f3fd0677345b2da76744e7e4ae77c20d68485bc79ae6cfb88e0ece58c04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
