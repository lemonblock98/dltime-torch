{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/shizhaoshu/project/dltime-torch\n"
     ]
    }
   ],
   "source": [
    "print(sys.path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "from dltime.data.tsc_dataset_names import *\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adiac',\n",
       " 'ArrowHead',\n",
       " 'BME',\n",
       " 'ChlorineConcentration',\n",
       " 'CricketX',\n",
       " 'CricketY',\n",
       " 'CricketZ',\n",
       " 'Crop',\n",
       " 'ECG200',\n",
       " 'ECG5000',\n",
       " 'Wafer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "univariate_equal_length_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_UCR_UEA_dataset(multivariate_equal_length[0], split=None, return_X_y=True, \\\n",
    "            extract_path=\"./ucr_uea_archive\") # x, y => Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_scaling(data, max_len):\n",
    "    \"\"\"\n",
    "    This is a function to scale the time series uniformly\n",
    "    :param data:\n",
    "    :param max_len:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    seq_len = len(data)\n",
    "    scaled_data = [data[int(j * seq_len / max_len)] for j in range(max_len)]\n",
    "\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X, min_len, normalise=None):\n",
    "    \"\"\"\n",
    "    This is a function to process the data, i.e. convert dataframe to numpy array\n",
    "    :param X:\n",
    "    :param min_len:\n",
    "    :param normalise:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tmp = []\n",
    "    seq_len = 0\n",
    "    for i in range(len(X)):\n",
    "        _x = X.iloc[i, :].copy(deep=True)\n",
    "        seq_len = max(seq_len, max([len(y) for y in _x]))\n",
    "\n",
    "    for i in tqdm(range(len(X))):\n",
    "        # 取出第i个数据\n",
    "        _x = X.iloc[i, :].copy(deep=True)\n",
    "\n",
    "        # 1. 计算该数据每一维的数据长度\n",
    "        all_len = [len(y) for y in _x]\n",
    "        max_len = max(all_len)\n",
    "\n",
    "        # 2. 统一每一维的数据长度\n",
    "        _y = []\n",
    "        for y in _x:\n",
    "            # 2.1 如果有缺失值, 进行插补\n",
    "            if y.isnull().any():\n",
    "                y = y.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "            # 2.2. 如果有维度间的数据长度不相等, 则填充到一致\n",
    "            if len(y) < max_len:\n",
    "                y = uniform_scaling(y, max_len)\n",
    "            _y.append(y)\n",
    "        _y = np.array(np.transpose(_y))\n",
    "\n",
    "        # 3. adjust the length of the series, chop of the longer series\n",
    "        # _y = _y[:min_len, :]\n",
    "\n",
    "        # 4. 归一化\n",
    "        if normalise == \"standard\":\n",
    "            scaler = StandardScaler().fit(_y)\n",
    "            _y = scaler.transform(_y)\n",
    "        if normalise == \"minmax\":\n",
    "            scaler = MinMaxScaler().fit(_y)\n",
    "            _y = scaler.transform(_y)\n",
    "\n",
    "        tmp.append(_y)\n",
    "    X = np.array(tmp)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:00<00:00, 838.86it/s]\n"
     ]
    }
   ],
   "source": [
    "x_ = process_data(x, min_len=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575, 144, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2ndarray(X):\n",
    "    \"X 是具体某一条数据, 而非整个数据集\"\n",
    "    # 1. 统计各维度的数据长度\n",
    "    all_len = [len(x) for x in X]\n",
    "    max_len = max(all_len)\n",
    "\n",
    "    # 2. 统一每一维度的数据长度\n",
    "    _X = []\n",
    "    for x in X:\n",
    "        # 2.1 如果有缺失值, 进行插补\n",
    "        if x.isnull().any():\n",
    "            x = x.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 2.2. 如果有维度间的数据长度不相等, 则填充到一致\n",
    "        if len(x) < max_len:\n",
    "            x = uniform_scaling(x, max_len)\n",
    "        _X.append(x)\n",
    "    _X = np.array(np.transpose(_X))\n",
    "\n",
    "    return _X\n",
    "\n",
    "def get_max_seq_len(data_df):\n",
    "    \"获取一个完整数据集中的最大序列长度\"\n",
    "    max_seq_len = 0\n",
    "    for i in range(len(data_df)):\n",
    "        # 取出第i个数据\n",
    "        X = data_df.iloc[i, :].copy(deep=True)\n",
    "        max_seq_len = max(max_seq_len, max([len(x) for x in X]))\n",
    "        return max_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tsMinMaxNormlizer:\n",
    "    \"用于对dataframe型的序列做最大最小归一化\"\n",
    "    def __init__(self, scale=(0, 1)):\n",
    "        self.scale = scale\n",
    "\n",
    "    def fit(self, X):\n",
    "        # 输入x为sktime型的dataframe\n",
    "        self.data_max_ = []\n",
    "        self.data_min_ = []\n",
    "        for dim in X.columns:\n",
    "            x = X[dim]\n",
    "            total_x = []\n",
    "            for _x in x:\n",
    "                total_x.extend(list(_x))\n",
    "            self.data_max_.append(max(total_x))\n",
    "            self.data_min_.append(min(total_x))\n",
    "\n",
    "    def transform(self, x):\n",
    "        # 输入x为numpy.array, x shape: (seq_len, dim)\n",
    "        result = []\n",
    "        for i in range(x.shape[-1]):\n",
    "            _x = x[:, i]\n",
    "            _x = (_x - self.data_min_[i]) / (self.data_max_[i] - self.data_min_[i])\n",
    "            _x = self.scale[0] + _x * (self.scale[1] - self.scale[0])\n",
    "            result.append(_x[:, np.newaxis])\n",
    "        \n",
    "        return np.concatenate(result, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCR_UEADataset(Dataset):\n",
    "    \"Torch Datasets for UCR/UEA archive\"\n",
    "\n",
    "    def __init__(self, name, split=None, extract_path=\"ucr_uea_archive\", max_len=256, return_y=True, mask=False, \n",
    "        normalize=None):\n",
    "        assert split in [\"train\", \"test\", None]\n",
    "        assert normalize in [\"standard\", \"minmax\", None]\n",
    "\n",
    "        super().__init__()\n",
    "        self.return_y = return_y\n",
    "        self.mask = mask\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.data, self.label = load_UCR_UEA_dataset(name, split=split, return_X_y=True, \\\n",
    "            extract_path=extract_path) # x, y => Dataframe\n",
    "        \n",
    "        self.max_len = max(max_len, get_max_seq_len(self.data) + 1) # 获取最大序列长度\n",
    "        self.normalizer = tsMinMaxNormlizer(scale=(0.05, 0.95))\n",
    "        self.normalizer.fit(self.data)\n",
    "\n",
    "        # 处理标签\n",
    "        self.label = np.array(self.label) # label 为具体标签的名称\n",
    "        self.label2y = dict([(y, i) for i, y in enumerate(np.unique(self.label))]) # 标签名与其对应的数字标签\n",
    "        self.y2label = list(self.label2y.values()) # 数字标签所对应的标签名\n",
    "        self.y = [self.label2y[label] for label in self.label] # 转换具体标签至标签名\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data.iloc[idx].copy(deep=True)\n",
    "        X = dataframe2ndarray(X)    # dataframe 转 numpy 数组\n",
    "\n",
    "        # 数据归一化, 均按维度进行归一化\n",
    "        X = self.normalizer.transform(X)\n",
    "        padding_mask = [0] + [0] * X.shape[0] + [1] * (self.max_len - X.shape[0] - 1)\n",
    "\n",
    "        cls = np.ones((1, X.shape[-1])) # [CLS]\n",
    "        pad = np.zeros((self.max_len - X.shape[0] - 1, X.shape[-1])) # [PAD]\n",
    "        X = np.concatenate([cls, X, pad], axis=0)\n",
    "        item = {\"input\": torch.from_numpy(X).float(), \"padding_mask\": torch.LongTensor(padding_mask)}\n",
    "\n",
    "        if self.return_y:\n",
    "            item[\"label\"] = torch.tensor(self.y[idx]).long()\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UCR_UEADataset(multivariate_equal_length[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['padding_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['input'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltime.models.ts_transformer import TSTransformerEncoderClassifier\n",
    "from dltime.classifiers import FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSTransformerEncoderClassifier(feat_dim=dataset[0]['input'].shape[-1], max_len=dataset.max_len, d_model=128, n_heads=4, num_layers=2, dim_feedforward=512, num_classes=len(dataset.y2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256, 9])\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(dataloader):\n",
    "    print(item['input'].size())\n",
    "    print(item['padding_mask'])\n",
    "    outputs = model(item['input'], item['padding_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21bb49273b3bc7e970573143769dbe2f8828a1cab3d00aefeffccffd0bb6ba7c"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
