{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dltime.base.layers import Conv1dSame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    自注意力机制\n",
    "    - q: query, shape: [..., seq_len_q, depth]\n",
    "    - k: key, shape: [..., seq_len_k, depth]\n",
    "    - v: value, shape: [..., seq_len_v, depth_v], seq_len_k == seq_len_v\n",
    "    有seq_len_q个query, seq_len_k个key, 计算其注意力值及其输出\n",
    "    \"\"\"\n",
    "    # q, k做矩阵乘法, 得到各个query查询各个key得到的value\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2)) # [..., seq_len_q, seq_len_k]\n",
    "    \n",
    "    # 将得到的value除以sqrt(d_k), 使其不至于太大, 不然输入到softmax后容易导致梯度消失\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32) # d_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "    # 需要 mask 的位置加上一个很大的负值, 使其输入到softmax之后对应概率为0\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9).unsqueeze(-2)\n",
    "    \n",
    "    # 计算Attention权重矩阵\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1) # [..., seq_len_q, seq_len_k]\n",
    "    \n",
    "    # 各个value按Attention矩阵加权, 得到各个query对应的最终输出\n",
    "    output = torch.matmul(attention_weights, v) # [..., seq_len_q, depth_v]\n",
    "    return output, attention_weights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_out=256, filter_map={1: 32, 3: 32, 5: 64, 7: 64, 9: 32, 11: 32}):\n",
    "        super(ConvSelfAttention, self).__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.filter_map = filter_map\n",
    "\n",
    "        self.wq = Conv1dSame(c_in, c_out, ks=1, stride=1)\n",
    "        self.wk = nn.ModuleList([Conv1dSame(c_in, co, ks=ks, stride=1) for ks, co in filter_map.items()])\n",
    "        self.wv = nn.ModuleList([Conv1dSame(c_in, co, ks=ks, stride=1) for ks, co in filter_map.items()])\n",
    "\n",
    "        self.final_linear = nn.Linear(c_out, c_out)\n",
    "\n",
    "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "\n",
    "        q = self.wq(q)  # =>[bs, d_model, seq_len]\n",
    "        k = torch.cat([conv(k) for conv in self.wk], dim=1)  # =>[bs, d_model, seq_len]\n",
    "        v = torch.cat([conv(v) for conv in self.wv], dim=1)  # =>[bs, d_model, seq_len]\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\\\n",
    "            q.transpose(-1, -2), k.transpose(-1, -2), v.transpose(-1, -2), mask)\n",
    "        # => [b, seq_len_q, d_model], [b, seq_len_q, d_model]\n",
    "        output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output.transpose(-1, -2), attention_weights  # [b, d_model, seq_len], [b, seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise ValueError(\"activation should be relu/gelu, not {}\".format(activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 96])\n",
      "torch.Size([64, 512, 96])\n",
      "torch.Size([64, 512, 96])\n",
      "torch.Size([64, 512, 96])\n",
      "torch.Size([64, 512, 96]) torch.Size([64, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "model = ConvSelfAttention(c_in=6, c_out=512)\n",
    "x = torch.rand(64, 6, 96) # [b,seq_len,d_model,embedding_dim]\n",
    "print(x.shape)\n",
    "out, attn_weights = model(x, x, x, mask=None)\n",
    "print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConvAttnBNEncoderLayer(nn.Module):\n",
    "    r\"\"\"This transformer encoder layer block is made up of self-attn and feedforward network.\n",
    "    It differs from TransformerEncoderLayer in torch/nn/modules/transformer.py in that it replaces LayerNorm\n",
    "    with BatchNorm.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model=256, dim_feedforward=512, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerConvAttnBNEncoderLayer, self).__init__()\n",
    "        self.self_attn = ConvSelfAttention(d_model, d_model)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm1d(d_model, eps=1e-5)  # normalizes each feature across batch samples and time steps\n",
    "        self.norm2 = nn.BatchNorm1d(d_model, eps=1e-5)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerConvAttnBNEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)  # (batch_size, d_model, seq_len)\n",
    "        src = self.norm1(src)\n",
    "        src = src.permute(0, 2, 1)  # restore (batch_size, seq_len, d_model)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)  # (batch_size, d_model, d_model)\n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = self.norm2(src)       # (batch_size, d_model, seq_len)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 96])\n",
      "torch.Size([64, 256, 96])\n"
     ]
    }
   ],
   "source": [
    "model = TransformerConvAttnBNEncoderLayer(d_model=256)\n",
    "x = torch.rand(64, 256, 96) # [b,seq_len,d_model,embedding_dim]\n",
    "print(x.shape)\n",
    "out = model(x)\n",
    "print(out.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltime.models.ts_transformer import get_pos_encoder\n",
    "class TSTransformerEncoderGAPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplest classifier/regressor. Can be either regressor or classifier because the output does not include\n",
    "    softmax. Concatenates final layer embeddings and uses 0s to ignore padding embeddings in final output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feat_dim, max_len, d_model, num_layers, dim_feedforward, num_classes,\n",
    "                 dropout=0.1, pos_encoding='fixed', activation='gelu', freeze=False):\n",
    "        super(TSTransformerEncoderGAPClassifier, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model, dropout=dropout*(1.0 - freeze), max_len=max_len)\n",
    "\n",
    "        encoder_layer = TransformerConvAttnBNEncoderLayer(d_model, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.act = _get_activation_fn(activation)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.output_layer = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, X, padding_masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, feat_dim, seq_len) torch tensor of masked features (input)\n",
    "            padding_masks: (batch_size, seq_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "        Returns:\n",
    "            output: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        inp = X.permute(0, 2, 1)    # [bs, seq_len, d_in]\n",
    "        inp = self.project_inp(inp) * math.sqrt(\n",
    "            self.d_model)           # [bs, seq_len, d_model]\n",
    "        inp = inp.permute(1, 0, 2)  # [seq_len, bs, d_model]\n",
    "        inp = self.pos_enc(inp)     # add positional encoding\n",
    "        inp = inp.permute(1, 2, 0)  # [bs, d_model, seq_len]\n",
    "\n",
    "        output = self.transformer_encoder(inp, src_key_padding_mask=padding_masks)  # (batch_size, d_model, seq_length)\n",
    "        output = self.act(output)   # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        # Output\n",
    "        gap_weight = F.softmax(padding_masks * -1e9, dim=-1).unsqueeze(-1)\n",
    "        output = torch.bmm(output, gap_weight).squeeze()\n",
    "        output = self.output_layer(output)  # (batch_size, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs, seq_len, 1 bs, d_model, seq_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 384]) torch.Size([64, 384])\n",
      "torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "model = TSTransformerEncoderGAPClassifier(feat_dim=6, max_len=384, d_model=256, num_layers=2, dim_feedforward=512, num_classes=3)\n",
    "x = torch.rand(64, 6, 384) # [b,seq_len,d_model,embedding_dim]\n",
    "mask = torch.cat([torch.zeros(64, 96), torch.ones(64, 384-96)], dim=1)\n",
    "print(x.size(), mask.size())\n",
    "out = model(x, mask)\n",
    "print(out.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21bb49273b3bc7e970573143769dbe2f8828a1cab3d00aefeffccffd0bb6ba7c"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
