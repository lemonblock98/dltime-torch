{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from data_process import *\n",
    "from data_split import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mygenerate_data(root_path, by_txt=True, shuffle=True, factor=0.2, snr=5, resample=True):\n",
    "    \"\"\"\n",
    "    根据打好标签的 txt 文件导入数据，并按文件来划分训练集以及测试集\n",
    "    其中训练集，测试集默认按 0.8 0.2 比例划分\n",
    "    数据集目录结构: area/data/, area/txt/\n",
    "    \"\"\"\n",
    "    data_root, txt_root = root_path + '/data', root_path + '/txt'\n",
    "    train_data, test_data = [], []\n",
    "    file_data_dict = {}\n",
    "\n",
    "    file_name_list = os.listdir(data_root)\n",
    "\n",
    "    for file_name in file_name_list:\n",
    "        \n",
    "        file_path = data_root + '/' + file_name\n",
    "        \n",
    "        dataXYZ = pd.read_csv(file_path, header= 0)\n",
    "\n",
    "        data_x, data_y, data_z = list(dataXYZ.iloc[:,0]), list(dataXYZ.iloc[:, 1]), list(dataXYZ.iloc[:, 2])\n",
    "        \n",
    "        if resample:\n",
    "            data_x, data_y, data_z = data_resample(data_x, 2, 1), data_resample(data_y, 2, 1), \\\n",
    "                data_resample(data_z, 2, 1)\n",
    "        \n",
    "        dataXYZ = pd.DataFrame()\n",
    "        dataXYZ['x'] = data_x\n",
    "        dataXYZ['y'] = data_y\n",
    "        dataXYZ['z'] = data_z\n",
    "\n",
    "        base_value = cal_base_value(dataXYZ, 16, 8, 200)\n",
    "        \n",
    "        if by_txt:\n",
    "            txt_path = txt_root + '/' + file_name[:-3] + 'txt'\n",
    "            with open(txt_path, 'r') as f:\n",
    "                activity_list = f.readlines()\n",
    "            activity_list = [int(activity[:-1]) for activity in activity_list]\n",
    "        else:\n",
    "            activity_list = [int(np.mean(idx)) for idx in activitySplit(dataXYZ, 16, 8, 200)]\n",
    "\n",
    "        new_list = []\n",
    "        for center in activity_list:\n",
    "            item = {'data_x': np.array(extract_data_from_center(data_x, center, base_value[0], length=64)),\n",
    "                    'data_y': np.array(extract_data_from_center(data_y, center, base_value[1], length=64)),\n",
    "                    'data_z': np.array(extract_data_from_center(data_z, center, base_value[2], length=64)),\n",
    "                    'label': get_activity_label(file_name), 'file_name': file_name, 'base_value':base_value,\n",
    "                    'angle': cal_angles(base_value), 'area': get_area_label(root_path) }\n",
    "            \n",
    "            noise_z = np.array(extract_data_from_center(data_z, center+32, base_value[2], 64))\n",
    "            item['snr'] = cal_snr(item['data_z']-item['base_value'][2], noise_z-item['base_value'][2])\n",
    "\n",
    "            \n",
    "            new_list.append(item)\n",
    "        # activity_list = [{'data_x': np.array(extract_data_from_center(data_x, center, base_value[0])),\n",
    "        #                 'data_y': np.array(extract_data_from_center(data_y, center, base_value[1])),\n",
    "        #                 'data_z': np.array(extract_data_from_center(data_z, center, base_value[2])),\n",
    "        #                 'label': get_activity_label(file_name), 'file_name': file_name, 'base_value':base_value,\n",
    "        #                 'angle': cal_angles(base_value), 'area': get_area_label(root_path) }\n",
    "        #                 for center in activity_list]\n",
    "        \n",
    "        file_data_dict[file_name] = filter_by_snr(new_list, snr)\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(new_list)\n",
    "        \n",
    "        test_data = test_data + new_list[: int(factor * len(new_list))]\n",
    "        train_data = train_data + new_list[int(factor * len(new_list)): ]\n",
    "        \n",
    "    return filter_by_snr(train_data, snr), filter_by_snr(test_data, snr), file_name_list, file_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_diff(sig):\n",
    "    return [sig[1]-sig[0]] + [sig[i+1] - sig[i] for i in range(len(sig)-1)]\n",
    "\n",
    "def newhandle_data_3dims(item, mode='origin'):\n",
    "    \"\"\"\n",
    "    将单个切割出来的数据处理按mode处理成三轴数\n",
    "    mode: 'origin'-只减基线，'combine'-转换为x2+y2+z2, x2+y2, z三轴数据\n",
    "    \"\"\"\n",
    "    base, angle = item['base_value'], item['angle'] # xyz的基线，以及其与g的夹角\n",
    "    data_x, data_y, data_z = item['data_x'], item['data_y'], item['data_z']\n",
    "    var = [0.00076385, 0.00017194, 0.00071417, 0.000022871, 0.000040234]\n",
    "\n",
    "    if mode == 'combine':\n",
    "        data_xyz = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2 + (data_z-base[2])**2) # x2+y2+z2不论如何都减基线\n",
    "        data_z_rectify = (data_x-base[0]) * angle['x'] + (data_y-base[1]) * angle['y'] + (data_z-base[2]) * angle['z']\n",
    "        data_xy = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2)\n",
    "        \n",
    "        data = np.array([cut_mean(data_xyz) / np.sqrt(var[0]), cut_mean(data_xy) / np.sqrt(var[1]), \\\n",
    "            cut_mean(data_z_rectify) / np.sqrt(var[2])], dtype=np.float64)\n",
    "\n",
    "    elif mode == 'origin':\n",
    "        data = np.array([data_x-base[0], data_y-base[1], data_z-base[2]], dtype=np.float64)\n",
    "\n",
    "    elif mode == 'all':\n",
    "        data_xyz = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2 + (data_z-base[2])**2) # x2+y2+z2不论如何都减基线\n",
    "        data_z_rectify = (data_x-base[0]) * angle['x'] + (data_y-base[1]) * angle['y'] + (data_z-base[2]) * angle['z']\n",
    "        data_xy = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2)\n",
    "\n",
    "        data = np.array([cut_mean(data_xyz) / np.sqrt(var[0]), \\\n",
    "            cut_mean(data_xy) / np.sqrt(var[1]), \\\n",
    "            cut_mean(data_z_rectify) / np.sqrt(var[2]), \\\n",
    "            cut_mean(data_x) / np.sqrt(var[3]), \\\n",
    "            cut_mean(data_y) / np.sqrt(var[4])], dtype=np.float64)\n",
    "\n",
    "    elif mode == 'diff':\n",
    "        diff_x, diff_y, diff_z = sig_diff(data_x), sig_diff(data_y), sig_diff(data_z)\n",
    "        data = np.array([diff_x, diff_y, diff_z])\n",
    "\n",
    "    elif mode == 'diff2':\n",
    "        data_xyz = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2 + (data_z-base[2])**2) # x2+y2+z2不论如何都减基线\n",
    "        data_z_rectify = (data_x-base[0]) * angle['x'] + (data_y-base[1]) * angle['y'] + (data_z-base[2]) * angle['z']\n",
    "        data_xy = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2)\n",
    "\n",
    "        diff_xyz, diff_g, diff_xy = sig_diff(data_xyz), sig_diff(data_z_rectify), sig_diff(data_xy)\n",
    "\n",
    "        data = np.array([diff_xyz, diff_g, diff_xy])\n",
    "\n",
    "    elif mode == 'diff3':\n",
    "        data_xyz = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2 + (data_z-base[2])**2) # x2+y2+z2不论如何都减基线\n",
    "        data_z_rectify = (data_x-base[0]) * angle['x'] + (data_y-base[1]) * angle['y'] + (data_z-base[2]) * angle['z']\n",
    "        data_xy = np.sqrt((data_x-base[0])**2 + (data_y-base[1])**2)\n",
    "\n",
    "        diff_xyz, diff_g, diff_xy = sig_diff(data_xyz), sig_diff(data_z_rectify), sig_diff(data_xy)\n",
    "\n",
    "        data = np.array([diff_xyz, diff_g, diff_xy, \\\n",
    "            cut_mean(data_xyz) / np.sqrt(var[0]), \\\n",
    "            cut_mean(data_xy) / np.sqrt(var[1]), \\\n",
    "            cut_mean(data_z_rectify) / np.sqrt(var[2])])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized mode: {}\".format(mode))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def newhandle_dataset_3dims(dataset, mode='origin'):\n",
    "    \"\"\"\n",
    "    对原始的数据进行处理，生成 data 与对应的 label\n",
    "    file_name_list: 需要用于生成数据集的文件名，在测试时可以选择几个文件单独生成数据集\n",
    "    mode: 'origin'-只减基线，'combine'-转换为x2+y2+z2, x2+y2, z三轴数据\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for item in dataset:\n",
    "        data.append(newhandle_data_3dims(item, mode))\n",
    "        label.append(item['label'])\n",
    "    \n",
    "    data = np.array(data, dtype=np.float64)\n",
    "    label = np.array(label)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data...\n",
      "generating data finishing...\n"
     ]
    }
   ],
   "source": [
    "snr=0\n",
    "print(\"generating data...\")\n",
    "syf_train, syf_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/syf', by_txt=False, snr=snr)\n",
    "syf2_train, syf2_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/syf2', by_txt=False, snr=snr)\n",
    "\n",
    "yqcc_train, yqcc_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/yqcc2', by_txt=False, snr=snr)\n",
    "yqcc2_train, yqcc2_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/yqcc2_md', by_txt=False, snr=snr)\n",
    "\n",
    "zwy_train, zwy_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/zwy', by_txt=False, snr=snr)\n",
    "zwy2_train, zwy2_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/zwy_d1', by_txt=False, snr=snr)\n",
    "zwy3_train, zwy3_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/zwy_418', by_txt=False, snr=snr)\n",
    "zwy4_train, zwy4_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/zwy_423', by_txt=False, snr=snr)\n",
    "zwy5_train, zwy5_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/zwy_621', by_txt=False, snr=snr)\n",
    "\n",
    "j11_train, j11_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/j11', by_txt=False, snr=snr)\n",
    "j11_2_train, j11_2_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/j11_328', by_txt=False, snr=snr)\n",
    "j11_md_train, j11_md_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/j11_49', by_txt=False, snr=snr)\n",
    "j11_527_train, j11_527_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/j11_527', by_txt=False, snr=snr)\n",
    "\n",
    "zyq_train, zyq_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/zyq', by_txt=False, snr=snr)\n",
    "zyq2_train, zyq2_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/zyq_d1', by_txt=False, snr=snr)\n",
    "\n",
    "j7lqc_train, j7lqc_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/j7lqc', by_txt=False, snr=snr)\n",
    "\n",
    "sky_train, sky_test, _, _ = mygenerate_data('D:/研一/嗑盐/土壤扰动/dataset/sky', by_txt=False, snr=snr)\n",
    "print(\"generating data finishing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = syf_train + syf2_train + yqcc_train + yqcc2_train + zwy_train + zwy2_train + zwy3_train + zwy4_train + \\\n",
    "    zwy5_train + j11_train + j11_2_train + j11_md_train + j11_527_train + j7lqc_train + sky_train\n",
    "\n",
    "test_data = syf_test + syf2_test + yqcc_test + yqcc2_test + zwy_test + zwy2_test + zwy3_test + zwy4_test + \\\n",
    "    zwy5_test + j11_test + j11_2_test + j11_md_test + j11_527_test + j7lqc_test + sky_test\n",
    "\n",
    "# train_data = zwy_train + zwy2_train + zwy3_train + zwy4_train\n",
    "# test_data = zwy_test + zwy2_test + zwy3_test + zwy4_test\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_label = newhandle_dataset_3dims(train_data, mode=\"all\")\n",
    "test_x, test_label = newhandle_dataset_3dims(test_data, mode=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19389, 64, 5) (19389,)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.swapaxes(train_x, 2, 1)\n",
    "test_x = np.swapaxes(test_x, 2, 1)\n",
    "print(train_x.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 4, 7])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1, 2, 3], [3, 4, 5]], [[6, 2, 3], [3, 4, 7]]])\n",
    "print(a.shape)\n",
    "np.max(a.reshape(-1, a.shape[-1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tsMinMaxNormlizer:\n",
    "    \"用于对dataframe型的序列做最大最小归一化\"\n",
    "    def __init__(self, scale=(0, 1)):\n",
    "        self.scale = scale\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        self.data_max_ = np.max(X.reshape(-1, X.shape[-1]), axis=0)\n",
    "        self.data_min_ = np.min(X.reshape(-1, X.shape[-1]), axis=0)\n",
    "\n",
    "    def transform(self, x):\n",
    "        # 输入x为numpy.array, x shape: (seq_len, dim)\n",
    "        result = []\n",
    "        for i in range(x.shape[-1]):\n",
    "            _x = x[:, i]\n",
    "            _x = (_x - self.data_min_[i]) / (self.data_max_[i] - self.data_min_[i])\n",
    "            _x = self.scale[0] + _x * (self.scale[1] - self.scale[0])\n",
    "            result.append(_x[:, np.newaxis])\n",
    "        \n",
    "        return np.concatenate(result, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tsMinMaxNormlizer(scale=(0.05, 0.95))\n",
    "norm.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltime.data.ts_datasets import noise_mask\n",
    "class MLM_Soil_Dataset(Dataset):\n",
    "    \"Torch Datasets for UCR/UEA archive\"\n",
    "\n",
    "    def __init__(self, data, add_cls=True, pt_ratio=0.5, max_len=128, normalize=None, \\\n",
    "        masking_ratio=0.2, lm=5, mode='separate', distribution='geometric'):\n",
    "\n",
    "        assert normalize in [\"standard\", \"minmax\", None]\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.add_cls = add_cls\n",
    "        self.pt_ratio = pt_ratio\n",
    "        self.max_len = max_len\n",
    "        self.normalize = normalize\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.lm = lm\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "        self.normalizer = tsMinMaxNormlizer(scale=(0.05, 0.95))\n",
    "        self.normalizer.fit(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx]\n",
    "\n",
    "        # 数据归一化, 均按维度进行归一化\n",
    "        X = self.normalizer.transform(X)\n",
    "        # padding mask\n",
    "        padding_mask = [0] + [0] * X.shape[0] + [1] * (self.max_len - X.shape[0] - 1)\n",
    "        # lm mask\n",
    "        lm_mask = ~noise_mask(X, self.masking_ratio, self.lm, self.mode, self.distribution)\n",
    "\n",
    "        cls = np.ones((1, X.shape[-1])) # [CLS]\n",
    "        pad = np.zeros((self.max_len - X.shape[0] - 1, X.shape[-1])) # [PAD]\n",
    "        X = np.concatenate([cls, X, pad], axis=0)\n",
    "        \n",
    "        # lm_mask\n",
    "        cls_mask = np.zeros((1, X.shape[-1]), dtype=np.bool) # [CLS]\n",
    "        pad_mask = pad[:]\n",
    "        lm_mask = torch.from_numpy(np.concatenate([cls_mask, lm_mask, pad_mask], axis=0)).bool()\n",
    "\n",
    "        item = {\"input\": torch.from_numpy(X[:]).masked_fill(lm_mask, -1).float(), \\\n",
    "            \"padding_mask\": torch.tensor(padding_mask).bool(), \n",
    "            \"output\": torch.from_numpy(X[:]).float(), \n",
    "            \"lm_mask\": lm_mask}\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mlm_dataset = MLM_Soil_Dataset(train_x)\n",
    "test_mlm_dataset = MLM_Soil_Dataset(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "from dltime.models.ts_transformer import TSTransformerEncoderMLM\n",
    "from config import TrainConfig\n",
    "from utils import get_logger, get_scheduler\n",
    "from transformers import AdamW\n",
    "from train_helper import mlm_train_fn, mlm_valid_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = TrainConfig()\n",
    "CFG.encoder_lr, CFG.decoder_lr = 2e-3, 2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mlm_dataloader = DataLoader(train_mlm_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
    "test_mlm_dataloader = DataLoader(test_mlm_dataset, batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Training =========\n"
     ]
    }
   ],
   "source": [
    "LOGGER = get_logger(\"soil_mlm_train\")\n",
    "LOGGER.info(f'========= Training =========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = train_x.shape[-1]\n",
    "max_len = 128\n",
    "model = TSTransformerEncoderMLM(\n",
    "    feat_dim=feat_dim, \n",
    "    max_len=max_len,\n",
    "    d_model=64, n_heads=2, num_layers=2, \n",
    "    dim_feedforward=512).to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "optimizer_parameters = model.parameters()\n",
    "    # optimizer_parameters = get_optimizer_params(model, CFG.encoder_lr, CFG.decoder_lr)\n",
    "optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "num_train_steps = int(len(train_mlm_dataset) / CFG.batch_size * CFG.epochs)\n",
    "scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "best_score = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1212] Elapsed 0m 0s (remain 0m 49s) Loss: 1.0210(1.0210) Grad: 1021.0754  LR: 0.00009987  \n",
      "Epoch: [1][100/1212] Elapsed 0m 1s (remain 0m 18s) Loss: 1.1395(1.0532) Grad: 1011.2743  LR: 0.00009986  \n",
      "Epoch: [1][200/1212] Elapsed 0m 3s (remain 0m 16s) Loss: 1.0538(1.0558) Grad: 1010.4191  LR: 0.00009985  \n",
      "Epoch: [1][300/1212] Elapsed 0m 4s (remain 0m 14s) Loss: 1.1217(1.0507) Grad: 1010.1683  LR: 0.00009984  \n",
      "Epoch: [1][400/1212] Elapsed 0m 6s (remain 0m 12s) Loss: 1.0660(1.0523) Grad: 1010.5606  LR: 0.00009982  \n",
      "Epoch: [1][500/1212] Elapsed 0m 7s (remain 0m 10s) Loss: 1.0106(1.0514) Grad: 1010.7153  LR: 0.00009981  \n",
      "Epoch: [1][600/1212] Elapsed 0m 9s (remain 0m 9s) Loss: 1.0564(1.0551) Grad: 1009.8890  LR: 0.00009980  \n",
      "Epoch: [1][700/1212] Elapsed 0m 10s (remain 0m 7s) Loss: 0.9709(1.0548) Grad: 1010.2654  LR: 0.00009979  \n",
      "Epoch: [1][800/1212] Elapsed 0m 12s (remain 0m 6s) Loss: 1.1066(1.0558) Grad: 1011.2684  LR: 0.00009978  \n",
      "Epoch: [1][900/1212] Elapsed 0m 13s (remain 0m 4s) Loss: 1.0242(1.0561) Grad: 1010.4534  LR: 0.00009977  \n",
      "Epoch: [1][1000/1212] Elapsed 0m 15s (remain 0m 3s) Loss: 1.1151(1.0546) Grad: 1011.0063  LR: 0.00009975  \n",
      "Epoch: [1][1100/1212] Elapsed 0m 18s (remain 0m 1s) Loss: 1.0465(1.0537) Grad: 1010.3734  LR: 0.00009974  \n",
      "Epoch: [1][1200/1212] Elapsed 0m 20s (remain 0m 0s) Loss: 1.0484(1.0547) Grad: 1009.7560  LR: 0.00009973  \n",
      "Epoch: [1][1211/1212] Elapsed 0m 20s (remain 0m 0s) Loss: 1.0419(1.0548) Grad: 1011.1822  LR: 0.00009973  \n",
      "EVAL: [0/294] Elapsed 0m 0s (remain 0m 1s) Loss: 0.9536(0.9536) \n",
      "EVAL: [100/294] Elapsed 0m 0s (remain 0m 1s) Loss: 0.9160(0.9304) \n",
      "EVAL: [200/294] Elapsed 0m 1s (remain 0m 0s) Loss: 0.9276(0.9308) \n",
      "EVAL: [293/294] Elapsed 0m 2s (remain 0m 0s) Loss: 0.9641(0.9292) \n",
      "Epoch 1 - avg_train_loss: 1.0548  avg_val_loss: 0.9292  time: 22s\n",
      "Epoch: [2][0/1212] Elapsed 0m 0s (remain 0m 19s) Loss: 1.0171(1.0171) Grad: 1010.1454  LR: 0.00009973  \n",
      "Epoch: [2][100/1212] Elapsed 0m 1s (remain 0m 18s) Loss: 1.1912(1.0596) Grad: 1010.8748  LR: 0.00009971  \n",
      "Epoch: [2][200/1212] Elapsed 0m 3s (remain 0m 18s) Loss: 1.0745(1.0629) Grad: 1010.5380  LR: 0.00009970  \n",
      "Epoch: [2][300/1212] Elapsed 0m 5s (remain 0m 17s) Loss: 0.9948(1.0593) Grad: 1010.2718  LR: 0.00009968  \n",
      "Epoch: [2][400/1212] Elapsed 0m 7s (remain 0m 15s) Loss: 0.9459(1.0583) Grad: 1009.8952  LR: 0.00009967  \n",
      "Epoch: [2][500/1212] Elapsed 0m 9s (remain 0m 13s) Loss: 1.0121(1.0564) Grad: 1010.1907  LR: 0.00009965  \n",
      "Epoch: [2][600/1212] Elapsed 0m 11s (remain 0m 11s) Loss: 0.9981(1.0571) Grad: 1010.5969  LR: 0.00009964  \n",
      "Epoch: [2][700/1212] Elapsed 0m 12s (remain 0m 9s) Loss: 1.1228(1.0577) Grad: 1011.4783  LR: 0.00009962  \n",
      "Epoch: [2][800/1212] Elapsed 0m 15s (remain 0m 8s) Loss: 1.1147(1.0577) Grad: 1010.1994  LR: 0.00009961  \n",
      "Epoch: [2][900/1212] Elapsed 0m 17s (remain 0m 6s) Loss: 1.0070(1.0572) Grad: 1010.4583  LR: 0.00009959  \n",
      "Epoch: [2][1000/1212] Elapsed 0m 19s (remain 0m 4s) Loss: 0.9795(1.0572) Grad: 1009.6853  LR: 0.00009957  \n",
      "Epoch: [2][1100/1212] Elapsed 0m 22s (remain 0m 2s) Loss: 1.0971(1.0572) Grad: 1010.5292  LR: 0.00009956  \n",
      "Epoch: [2][1200/1212] Elapsed 0m 24s (remain 0m 0s) Loss: 1.0303(1.0582) Grad: 1010.9504  LR: 0.00009954  \n",
      "Epoch: [2][1211/1212] Elapsed 0m 24s (remain 0m 0s) Loss: 1.0895(1.0580) Grad: 1011.0212  LR: 0.00009954  \n",
      "EVAL: [0/294] Elapsed 0m 0s (remain 0m 2s) Loss: 0.9253(0.9253) \n",
      "EVAL: [100/294] Elapsed 0m 0s (remain 0m 1s) Loss: 0.9567(0.9312) \n",
      "EVAL: [200/294] Elapsed 0m 1s (remain 0m 0s) Loss: 0.9386(0.9319) \n",
      "EVAL: [293/294] Elapsed 0m 2s (remain 0m 0s) Loss: 0.9868(0.9330) \n",
      "Epoch 2 - avg_train_loss: 1.0580  avg_val_loss: 0.9330  time: 27s\n",
      "Epoch: [3][0/1212] Elapsed 0m 0s (remain 0m 23s) Loss: 1.1577(1.1577) Grad: 1010.3477  LR: 0.00009954  \n",
      "Epoch: [3][100/1212] Elapsed 0m 1s (remain 0m 17s) Loss: 1.0463(1.0692) Grad: 1011.0743  LR: 0.00009952  \n",
      "Epoch: [3][200/1212] Elapsed 0m 4s (remain 0m 21s) Loss: 1.0806(1.0630) Grad: 1010.9249  LR: 0.00009950  \n",
      "Epoch: [3][300/1212] Elapsed 0m 6s (remain 0m 20s) Loss: 1.2052(1.0615) Grad: 1011.2009  LR: 0.00009948  \n",
      "Epoch: [3][400/1212] Elapsed 0m 8s (remain 0m 16s) Loss: 1.2582(1.0634) Grad: 1010.3327  LR: 0.00009946  \n",
      "Epoch: [3][500/1212] Elapsed 0m 9s (remain 0m 13s) Loss: 0.9523(1.0630) Grad: 1009.4745  LR: 0.00009944  \n",
      "Epoch: [3][600/1212] Elapsed 0m 12s (remain 0m 12s) Loss: 1.1391(1.0633) Grad: 1009.7911  LR: 0.00009943  \n",
      "Epoch: [3][700/1212] Elapsed 0m 14s (remain 0m 10s) Loss: 1.0368(1.0629) Grad: 1010.3578  LR: 0.00009941  \n",
      "Epoch: [3][800/1212] Elapsed 0m 16s (remain 0m 8s) Loss: 1.0580(1.0602) Grad: 1010.1426  LR: 0.00009939  \n",
      "Epoch: [3][900/1212] Elapsed 0m 18s (remain 0m 6s) Loss: 1.0781(1.0598) Grad: 1010.3879  LR: 0.00009937  \n",
      "Epoch: [3][1000/1212] Elapsed 0m 22s (remain 0m 4s) Loss: 1.1383(1.0611) Grad: 1011.3600  LR: 0.00009934  \n",
      "Epoch: [3][1100/1212] Elapsed 0m 23s (remain 0m 2s) Loss: 1.1904(1.0597) Grad: 1011.2522  LR: 0.00009932  \n",
      "Epoch: [3][1200/1212] Elapsed 0m 25s (remain 0m 0s) Loss: 1.1036(1.0586) Grad: 1010.6609  LR: 0.00009930  \n",
      "Epoch: [3][1211/1212] Elapsed 0m 25s (remain 0m 0s) Loss: 1.0167(1.0587) Grad: 1010.3586  LR: 0.00009930  \n",
      "EVAL: [0/294] Elapsed 0m 0s (remain 0m 2s) Loss: 0.9143(0.9143) \n",
      "EVAL: [100/294] Elapsed 0m 2s (remain 0m 4s) Loss: 0.9373(0.9276) \n",
      "EVAL: [200/294] Elapsed 0m 4s (remain 0m 1s) Loss: 0.9270(0.9285) \n",
      "EVAL: [293/294] Elapsed 0m 4s (remain 0m 0s) Loss: 0.8225(0.9263) \n",
      "Epoch 3 - avg_train_loss: 1.0587  avg_val_loss: 0.9263  time: 31s\n",
      "Epoch: [4][0/1212] Elapsed 0m 0s (remain 0m 28s) Loss: 1.1665(1.1665) Grad: 1011.3372  LR: 0.00009930  \n",
      "Epoch: [4][100/1212] Elapsed 0m 1s (remain 0m 21s) Loss: 1.1062(1.0544) Grad: 1011.0928  LR: 0.00009928  \n",
      "Epoch: [4][200/1212] Elapsed 0m 3s (remain 0m 17s) Loss: 1.1106(1.0567) Grad: 1010.8199  LR: 0.00009926  \n",
      "Epoch: [4][300/1212] Elapsed 0m 5s (remain 0m 16s) Loss: 1.1050(1.0565) Grad: 1010.9357  LR: 0.00009923  \n",
      "Epoch: [4][400/1212] Elapsed 0m 8s (remain 0m 17s) Loss: 1.0972(1.0581) Grad: 1011.2471  LR: 0.00009921  \n",
      "Epoch: [4][500/1212] Elapsed 0m 10s (remain 0m 14s) Loss: 0.9904(1.0549) Grad: 1009.6042  LR: 0.00009919  \n",
      "Epoch: [4][600/1212] Elapsed 0m 11s (remain 0m 12s) Loss: 0.9943(1.0538) Grad: 1009.9529  LR: 0.00009916  \n",
      "Epoch: [4][700/1212] Elapsed 0m 14s (remain 0m 10s) Loss: 0.9485(1.0543) Grad: 1009.9550  LR: 0.00009914  \n",
      "Epoch: [4][800/1212] Elapsed 0m 17s (remain 0m 8s) Loss: 1.0223(1.0548) Grad: 1010.2646  LR: 0.00009912  \n",
      "Epoch: [4][900/1212] Elapsed 0m 18s (remain 0m 6s) Loss: 1.2534(1.0564) Grad: 1012.2651  LR: 0.00009909  \n",
      "Epoch: [4][1000/1212] Elapsed 0m 20s (remain 0m 4s) Loss: 1.0967(1.0576) Grad: 1011.0519  LR: 0.00009907  \n",
      "Epoch: [4][1100/1212] Elapsed 0m 23s (remain 0m 2s) Loss: 0.9963(1.0558) Grad: 1010.5060  LR: 0.00009904  \n",
      "Epoch: [4][1200/1212] Elapsed 0m 25s (remain 0m 0s) Loss: 0.9481(1.0551) Grad: 1010.1292  LR: 0.00009902  \n",
      "Epoch: [4][1211/1212] Elapsed 0m 25s (remain 0m 0s) Loss: 1.0337(1.0553) Grad: 1010.7681  LR: 0.00009901  \n",
      "EVAL: [0/294] Elapsed 0m 0s (remain 0m 2s) Loss: 0.9419(0.9419) \n",
      "EVAL: [100/294] Elapsed 0m 0s (remain 0m 1s) Loss: 0.9055(0.9318) \n",
      "EVAL: [200/294] Elapsed 0m 1s (remain 0m 0s) Loss: 0.9125(0.9316) \n",
      "EVAL: [293/294] Elapsed 0m 2s (remain 0m 0s) Loss: 0.9893(0.9321) \n",
      "Epoch 4 - avg_train_loss: 1.0553  avg_val_loss: 0.9321  time: 28s\n",
      "Epoch: [5][0/1212] Elapsed 0m 0s (remain 0m 16s) Loss: 0.9679(0.9679) Grad: 1009.2889  LR: 0.00009901  \n",
      "Epoch: [5][100/1212] Elapsed 0m 1s (remain 0m 21s) Loss: 0.9810(1.0674) Grad: 1009.3247  LR: 0.00009899  \n",
      "Epoch: [5][200/1212] Elapsed 0m 4s (remain 0m 22s) Loss: 0.9828(1.0654) Grad: 1009.9263  LR: 0.00009896  \n",
      "Epoch: [5][300/1212] Elapsed 0m 6s (remain 0m 19s) Loss: 0.9831(1.0584) Grad: 1010.8809  LR: 0.00009893  \n",
      "Epoch: [5][400/1212] Elapsed 0m 7s (remain 0m 16s) Loss: 1.1595(1.0552) Grad: 1010.6541  LR: 0.00009891  \n",
      "Epoch: [5][500/1212] Elapsed 0m 10s (remain 0m 14s) Loss: 1.0854(1.0565) Grad: 1010.5128  LR: 0.00009888  \n",
      "Epoch: [5][600/1212] Elapsed 0m 12s (remain 0m 13s) Loss: 1.0263(1.0543) Grad: 1009.8975  LR: 0.00009885  \n",
      "Epoch: [5][700/1212] Elapsed 0m 14s (remain 0m 10s) Loss: 0.9879(1.0558) Grad: 1009.7421  LR: 0.00009883  \n",
      "Epoch: [5][800/1212] Elapsed 0m 15s (remain 0m 8s) Loss: 0.8903(1.0562) Grad: 1008.9062  LR: 0.00009880  \n",
      "Epoch: [5][900/1212] Elapsed 0m 18s (remain 0m 6s) Loss: 1.0120(1.0567) Grad: 1009.8749  LR: 0.00009877  \n",
      "Epoch: [5][1000/1212] Elapsed 0m 20s (remain 0m 4s) Loss: 1.1513(1.0572) Grad: 1011.5856  LR: 0.00009874  \n",
      "Epoch: [5][1100/1212] Elapsed 0m 22s (remain 0m 2s) Loss: 1.1689(1.0579) Grad: 1010.9452  LR: 0.00009871  \n",
      "Epoch: [5][1200/1212] Elapsed 0m 23s (remain 0m 0s) Loss: 1.0202(1.0587) Grad: 1010.8872  LR: 0.00009868  \n",
      "Epoch: [5][1211/1212] Elapsed 0m 24s (remain 0m 0s) Loss: 1.0597(1.0586) Grad: 1009.8352  LR: 0.00009868  \n",
      "EVAL: [0/294] Elapsed 0m 0s (remain 0m 3s) Loss: 0.9418(0.9418) \n",
      "EVAL: [100/294] Elapsed 0m 1s (remain 0m 2s) Loss: 0.9072(0.9298) \n",
      "EVAL: [200/294] Elapsed 0m 2s (remain 0m 1s) Loss: 0.9317(0.9310) \n",
      "EVAL: [293/294] Elapsed 0m 3s (remain 0m 0s) Loss: 0.9737(0.9308) \n",
      "Epoch 5 - avg_train_loss: 1.0586  avg_val_loss: 0.9308  time: 28s\n",
      "Epoch: [6][0/1212] Elapsed 0m 0s (remain 0m 33s) Loss: 0.9199(0.9199) Grad: 1009.8206  LR: 0.00009868  \n",
      "Epoch: [6][100/1212] Elapsed 0m 1s (remain 0m 19s) Loss: 0.9243(1.0574) Grad: 1010.1029  LR: 0.00009865  \n",
      "Epoch: [6][200/1212] Elapsed 0m 3s (remain 0m 16s) Loss: 1.0963(1.0597) Grad: 1010.8217  LR: 0.00009862  \n",
      "Epoch: [6][300/1212] Elapsed 0m 5s (remain 0m 17s) Loss: 1.1039(1.0612) Grad: 1010.9578  LR: 0.00009859  \n",
      "Epoch: [6][400/1212] Elapsed 0m 8s (remain 0m 16s) Loss: 1.0274(1.0597) Grad: 1011.0281  LR: 0.00009856  \n",
      "Epoch: [6][500/1212] Elapsed 0m 9s (remain 0m 14s) Loss: 1.1154(1.0569) Grad: 1010.8215  LR: 0.00009853  \n",
      "Epoch: [6][600/1212] Elapsed 0m 11s (remain 0m 11s) Loss: 0.9673(1.0567) Grad: 1009.6929  LR: 0.00009849  \n",
      "Epoch: [6][700/1212] Elapsed 0m 14s (remain 0m 10s) Loss: 1.1046(1.0572) Grad: 1009.9148  LR: 0.00009846  \n",
      "Epoch: [6][800/1212] Elapsed 0m 16s (remain 0m 8s) Loss: 1.0394(1.0572) Grad: 1010.2054  LR: 0.00009843  \n",
      "Epoch: [6][900/1212] Elapsed 0m 18s (remain 0m 6s) Loss: 1.0074(1.0577) Grad: 1009.2330  LR: 0.00009840  \n",
      "Epoch: [6][1000/1212] Elapsed 0m 20s (remain 0m 4s) Loss: 0.9718(1.0572) Grad: 1010.5876  LR: 0.00009837  \n",
      "Epoch: [6][1100/1212] Elapsed 0m 23s (remain 0m 2s) Loss: 1.1014(1.0578) Grad: 1010.7092  LR: 0.00009833  \n",
      "Epoch: [6][1200/1212] Elapsed 0m 24s (remain 0m 0s) Loss: 1.0513(1.0578) Grad: 1010.7091  LR: 0.00009830  \n",
      "Epoch: [6][1211/1212] Elapsed 0m 24s (remain 0m 0s) Loss: 0.9873(1.0577) Grad: 1009.9437  LR: 0.00009830  \n",
      "EVAL: [0/294] Elapsed 0m 0s (remain 0m 2s) Loss: 0.9175(0.9175) \n",
      "EVAL: [100/294] Elapsed 0m 0s (remain 0m 1s) Loss: 0.8717(0.9169) \n",
      "EVAL: [200/294] Elapsed 0m 1s (remain 0m 0s) Loss: 0.9244(0.9194) \n",
      "EVAL: [293/294] Elapsed 0m 2s (remain 0m 0s) Loss: 0.9768(0.9183) \n",
      "Epoch 6 - avg_train_loss: 1.0577  avg_val_loss: 0.9183  time: 28s\n",
      "Epoch: [7][0/1212] Elapsed 0m 0s (remain 0m 34s) Loss: 1.0503(1.0503) Grad: 1010.7544  LR: 0.00009830  \n",
      "Epoch: [7][100/1212] Elapsed 0m 2s (remain 0m 25s) Loss: 1.1363(1.0514) Grad: 1010.6025  LR: 0.00009826  \n",
      "Epoch: [7][200/1212] Elapsed 0m 3s (remain 0m 19s) Loss: 1.1449(1.0579) Grad: 1010.8459  LR: 0.00009823  \n",
      "Epoch: [7][300/1212] Elapsed 0m 5s (remain 0m 17s) Loss: 1.0698(1.0586) Grad: 1010.2632  LR: 0.00009819  \n",
      "Epoch: [7][400/1212] Elapsed 0m 8s (remain 0m 17s) Loss: 1.0826(1.0580) Grad: 1010.7379  LR: 0.00009816  \n",
      "Epoch: [7][500/1212] Elapsed 0m 10s (remain 0m 14s) Loss: 1.1093(1.0555) Grad: 1010.7823  LR: 0.00009812  \n",
      "Epoch: [7][600/1212] Elapsed 0m 11s (remain 0m 12s) Loss: 0.9727(1.0537) Grad: 1009.6440  LR: 0.00009809  \n",
      "Epoch: [7][700/1212] Elapsed 0m 15s (remain 0m 11s) Loss: 1.1450(1.0531) Grad: 1010.5220  LR: 0.00009805  \n",
      "Epoch: [7][800/1212] Elapsed 0m 17s (remain 0m 8s) Loss: 0.8903(1.0534) Grad: 1010.0834  LR: 0.00009802  \n",
      "Epoch: [7][900/1212] Elapsed 0m 19s (remain 0m 6s) Loss: 1.1403(1.0536) Grad: 1011.1720  LR: 0.00009798  \n",
      "Epoch: [7][1000/1212] Elapsed 0m 21s (remain 0m 4s) Loss: 1.0686(1.0546) Grad: 1011.1990  LR: 0.00009794  \n",
      "Epoch: [7][1100/1212] Elapsed 0m 24s (remain 0m 2s) Loss: 1.0909(1.0525) Grad: 1010.5395  LR: 0.00009791  \n",
      "Epoch: [7][1200/1212] Elapsed 0m 26s (remain 0m 0s) Loss: 0.9765(1.0534) Grad: 1010.9244  LR: 0.00009787  \n",
      "Epoch: [7][1211/1212] Elapsed 0m 26s (remain 0m 0s) Loss: 0.9105(1.0536) Grad: 1009.2754  LR: 0.00009787  \n",
      "EVAL: [0/294] Elapsed 0m 0s (remain 0m 2s) Loss: 0.9533(0.9533) \n",
      "EVAL: [100/294] Elapsed 0m 0s (remain 0m 1s) Loss: 0.9582(0.9317) \n",
      "EVAL: [200/294] Elapsed 0m 2s (remain 0m 0s) Loss: 0.9188(0.9282) \n",
      "EVAL: [293/294] Elapsed 0m 3s (remain 0m 0s) Loss: 1.0133(0.9290) \n",
      "Epoch 7 - avg_train_loss: 1.0536  avg_val_loss: 0.9290  time: 29s\n",
      "Epoch: [8][0/1212] Elapsed 0m 0s (remain 0m 39s) Loss: 1.0665(1.0665) Grad: 1010.0086  LR: 0.00009786  \n",
      "Epoch: [8][100/1212] Elapsed 0m 2s (remain 0m 25s) Loss: 1.1644(1.0582) Grad: 1011.0729  LR: 0.00009783  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-46b78ee4c2c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlm_train_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mlm_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\dltime-torch\\test_program\\train_helper.py\u001b[0m in \u001b[0;36mmlm_train_fn\u001b[1;34m(cfg, train_loader, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(CFG.epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # train\n",
    "    avg_loss = mlm_train_fn(CFG, train_mlm_dataloader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n",
    "\n",
    "    # eval\n",
    "    avg_val_loss = mlm_valid_fn(CFG, test_mlm_dataloader, model, criterion, CFG.device)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "    \n",
    "    if best_score > avg_val_loss:\n",
    "        best_score = avg_val_loss\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "        torch.save(model.state_dict(), f\"outputs/soil_best.pth\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21bb49273b3bc7e970573143769dbe2f8828a1cab3d00aefeffccffd0bb6ba7c"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
